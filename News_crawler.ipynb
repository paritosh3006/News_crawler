{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser as fp\n",
    "import json\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from time import mktime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 4\n",
    "\n",
    "data = {}\n",
    "data['newspapers'] = {}\n",
    "\n",
    "# Loads the JSON files with news sites\n",
    "with open(r'C:\\Users\\Sony\\Desktop\\news.json') as data_file:\n",
    "    companies = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building site for  Hindustan Times\n",
      "1 articles downloaded from Hindustan Times  using newspaper, url:  https://www.hindustantimes.com/lok-sabha-elections/2019-lok-sabha-elections-ncp-chief-sharad-pawar-is-no-loser-in-maharashtra/story-xO04WYvyjAaPLKU4JPUBzH.html\n",
      "2 articles downloaded from Hindustan Times  using newspaper, url:  https://www.hindustantimes.com/opinion/lok-sabha-elections-2019-will-the-triangular-contest-work-to-bjp-s-advantage-in-uttar-pradesh/story-TKcULaIj24tFx6zfzYocSN.html\n",
      "3 articles downloaded from Hindustan Times  using newspaper, url:  https://www.hindustantimes.com/lok-sabha-elections/lok-sabha-elections-2019-bjp-s-last-minute-move-wins-over-disgruntled-allies/story-t65vchMTSGkvelYBJV3gHK.html\n",
      "4 articles downloaded from Hindustan Times  using newspaper, url:  https://www.hindustantimes.com/lok-sabha-elections/lok-sabha-elections-2019-bjp-to-reach-out-to-minorities-dalits-in-uttarakhand-as-part-of-poll-strategy/story-PRaw1bT8ZliIFTgFaAmLhL.html\n",
      "Downloading articles from  timesofindia\n",
      "Downloading articles from  India - BBC News\n",
      "1 articles downloaded from India - BBC News , url:  https://www.bbc.co.uk/news/world-asia-47479747\n",
      "2 articles downloaded from India - BBC News , url:  https://www.bbc.co.uk/news/world-asia-india-47114401\n",
      "3 articles downloaded from India - BBC News , url:  https://www.bbc.co.uk/news/world-asia-47536502\n",
      "4 articles downloaded from India - BBC News , url:  https://www.bbc.co.uk/news/business-47532522\n",
      "Building site for  The Indian Express\n",
      "1 articles downloaded from The Indian Express  using newspaper, url:  https://indianexpress.com/photos/e-p-unny-cartoons-gallery/business-as-usual-by-e-p-unny-march-2019-5606011/\n",
      "2 articles downloaded from The Indian Express  using newspaper, url:  https://indianexpress.com/article/india/terror-funding-case-ed-looks-at-role-of-pakistan-high-commission-in-disbursing-funds-5623473/\n",
      "3 articles downloaded from The Indian Express  using newspaper, url:  https://indianexpress.com/article/business/aviation/all-boeing-737-max-jets-to-be-grounded-by-4pm-today-dgca-5623714/\n",
      "4 articles downloaded from The Indian Express  using newspaper, url:  https://indianexpress.com/article/india/masood-azhar-listing-unsc-pakistan-india-us-jem-5623455/\n",
      "Building site for  firstpost\n",
      "1 articles downloaded from firstpost  using newspaper, url:  https://www.firstpost.com/tech/reviews/mobile/oppo-f11-pro-review-excellent-design-and-a-great-camera-make-it-easy-to-recommend-6241001.html\n",
      "2 articles downloaded from firstpost  using newspaper, url:  https://www.firstpost.com/politics/son-of-senior-opposition-leader-sujay-vikhe-patil-joins-bjp-family-known-for-turning-sides-but-scions-move-could-dent-cong-ncp-alliance-6248241.html\n",
      "3 articles downloaded from firstpost  using newspaper, url:  https://www.firstpost.com/politics/lok-sabha-polls-narendra-modi-does-not-campaign-he-tells-stories-in-which-opposition-is-villain-and-he-is-hero-6248171.html\n",
      "4 articles downloaded from firstpost  using newspaper, url:  https://www.firstpost.com/politics/sharad-pawar-says-bjp-may-win-most-seats-but-rules-out-possibility-of-second-term-for-narendra-modi-6248051.html\n",
      "Building site for  indiatimes\n",
      "1  Article has date of type None...\n",
      "2  Article has date of type None...\n",
      "3  Article has date of type None...\n",
      "4  Article has date of type None...\n",
      "Building site for  ndtv\n",
      "1  Article has date of type None...\n",
      "2  Article has date of type None...\n",
      "3  Article has date of type None...\n",
      "4  Article has date of type None...\n",
      "Downloading articles from  thehindu\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each news company\n",
    "for company, value in companies.items():\n",
    "    # If a RSS link is provided in the JSON file, this will be the first choice.\n",
    "    # Reason for this is that, RSS feeds often give more consistent and correct data.\n",
    "    # If you do not want to scrape from the RSS-feed, just leave the RSS attr empty in the JSON file.\n",
    "    if 'rss' in value:\n",
    "        d = fp.parse(value['rss'])\n",
    "        print(\"Downloading articles from \", company)\n",
    "        newsPaper = {\n",
    "            \"rss\": value['rss'],\n",
    "            \"link\": value['link'],\n",
    "            \"articles\": []\n",
    "        }\n",
    "        for entry in d.entries:\n",
    "            # Check if publish date is provided, if no the article is skipped.\n",
    "            # This is done to keep consistency in the data and to keep the script from crashing.\n",
    "            if hasattr(entry, 'published'):\n",
    "                if count > LIMIT:\n",
    "                    break\n",
    "                article = {}\n",
    "                article['link'] = entry.link\n",
    "                date = entry.published_parsed\n",
    "                article['published'] = datetime.fromtimestamp(mktime(date)).isoformat()\n",
    "                try:\n",
    "                    content = Article(entry.link)\n",
    "                    content.download()\n",
    "                    content.parse()\n",
    "                except Exception as e:\n",
    "                    # If the download for some reason fails (ex. 404) the script will continue downloading\n",
    "                    # the next article.\n",
    "                    print(e)\n",
    "                    print(\"continuing...\")\n",
    "                    continue\n",
    "                article['title'] = content.title\n",
    "            \n",
    "                newsPaper['articles'].append(article)\n",
    "                print(count, \"articles downloaded from\", company, \", url: \", entry.link)\n",
    "                count = count + 1\n",
    "    else:\n",
    "        # This is the fallback method if a RSS-feed link is not provided.\n",
    "        # It uses the python newspaper library to extract articles\n",
    "        print(\"Building site for \", company)\n",
    "        paper = newspaper.build(value['link'], memoize_articles=False)\n",
    "        newsPaper = {\n",
    "            \"link\": value['link'],\n",
    "            \"articles\": []\n",
    "        }\n",
    "        noneTypeCount = 0\n",
    "        for content in paper.articles:\n",
    "            if count > LIMIT:\n",
    "                break\n",
    "            try:\n",
    "                content.download()\n",
    "                content.parse()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"continuing...\")\n",
    "                continue\n",
    "            # Again, for consistency, if there is no found publish date the article will be skipped.\n",
    "            # After 10 downloaded articles from the same newspaper without publish date, the company will be skipped.\n",
    "            if content.publish_date is None:\n",
    "                print(count, \" Article has date of type None...\")\n",
    "                noneTypeCount = noneTypeCount + 1\n",
    "                if noneTypeCount > 10:\n",
    "                    print(\"Too many noneType dates, aborting...\")\n",
    "                    noneTypeCount = 0\n",
    "                    break\n",
    "                count = count + 1\n",
    "                continue\n",
    "            article = {}\n",
    "            article['title'] = content.title\n",
    "            \n",
    "            article['link'] = content.url\n",
    "            article['published'] = content.publish_date.isoformat()\n",
    "            newsPaper['articles'].append(article)\n",
    "            print(count, \"articles downloaded from\", company, \" using newspaper, url: \", content.url)\n",
    "            count = count + 1\n",
    "            noneTypeCount = 0\n",
    "    count = 1\n",
    "    data['newspapers'][company] = newsPaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(r'C:\\Users\\Sony\\Desktop\\nwsppr1.xls', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you want in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(r'C:\\Users\\Sony\\Desktop\\nws.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
